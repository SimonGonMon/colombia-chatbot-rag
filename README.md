# Colombia Chatbot RAG

Este proyecto implementa un chatbot especializado en responder preguntas sobre Colombia, utilizando informaci칩n extra칤da de Wikipedia. El sistema se basa en una arquitectura RAG (Retrieval-Augmented Generation) para proporcionar respuestas precisas y contextualizadas.

**Tecnolog칤as Principales:**
*   **Backend:** FastAPI
*   **Frontend:** Streamlit
*   **Orquestaci칩n LLM:** LangChain
*   **Modelos de Lenguaje:** OpenAI gpt-4o (principal) y gpt-4o-mini (apoyo)
*   **Embeddings:** OpenAI text-embedding-3-small
*   **Base de Datos Vectorial:** Pinecone
*   **Base de Datos Conversacional:** PostgreSQL
*   **Contenerizaci칩n:** Docker

## Capacidades Avanzadas del Chatbot

Este chatbot va m치s all치 de una simple respuesta, incorporando l칩gicas complejas para ofrecer una experiencia conversacional superior:

*   **Especializaci칩n por Tipo de Pregunta:** El chatbot adapta su "persona" (historiador, ge칩grafo, antrop칩logo cultural) seg칰n la intenci칩n de la pregunta, proporcionando respuestas m치s precisas y enfocadas.
*   **Respuestas Estructuradas:** Las respuestas se presentan en un formato claro y consistente, incluyendo una respuesta directa, detalles clave y contexto adicional, facilitando la comprensi칩n.
*   **Validaci칩n de Relevancia:** Solo responde preguntas relacionadas con Colombia. Si la pregunta est치 fuera de su dominio, lo indica amablemente.
*   **Complejidad de Respuesta Gradual:** Ajusta el nivel de detalle de la respuesta (breve, balanceada o detallada) seg칰n las palabras clave en la pregunta del usuario.
*   **Citas Inteligentes:** Proporciona la secci칩n espec칤fica de Wikipedia de donde se extrajo la informaci칩n, aumentando la transparencia y la confianza en las respuestas.
*   **Conversaciones con Estado (Stateful):** Gracias a su integraci칩n con una base de datos, el chatbot puede recordar el contexto de conversaciones pasadas, permitiendo di치logos fluidos y coherentes a lo largo del tiempo.

## C칩mo Ejecutar el Proyecto

**Nota Importante:** Si se te han proporcionado las variables de entorno necesarias (por ejemplo, como parte de una prueba t칠cnica 游떂), es probable que la base de datos vectorial (Pinecone) ya contenga los embeddings pre-generados. En este caso, simplemente ejecutar el proyecto con Docker Compose ser치 suficiente para ponerlo en marcha.

La forma m치s sencilla y recomendada de poner en marcha el proyecto completo es utilizando Docker Compose, ya que gestiona autom치ticamente la base de datos y las dependencias entre servicios.

### Usando Docker Compose (Recomendado)

Este m칠todo configura y ejecuta todos los servicios (API, Streamlit y PostgreSQL) con un solo comando.

1.  Aseg칰rate de tener Docker y Docker Compose instalados.
2.  Crea tu archivo `.env` a partir del `.env.example` y rellena tus propias credenciales para `OPENAI_API_KEY`, `PINECONE_API_KEY`, y `PINECONE_INDEX_NAME`. La `DATABASE_URL` y `API_BASE_URL` se configuran autom치ticamente en `docker-compose.yml`.
3.  En la ra칤z del proyecto, ejecuta el siguiente comando:
    ```bash
    docker-compose up --build
    ```
4.  Una vez que los contenedores est칠n en funcionamiento:
    *   Accede a la interfaz del chatbot en: **http://localhost:8501**
    *   La documentaci칩n interactiva de la API (Scalar) estar치 disponible en: **http://localhost:8000**

### Ejecuci칩n Local (Requiere Pasos Extra)

Si prefieres no usar Docker, puedes ejecutar la API y la aplicaci칩n de Streamlit por separado. Este m칠todo requiere una configuraci칩n manual de la base de datos y las variables de entorno.

#### Prerrequisitos y Configuraci칩n

1.  **Base de Datos PostgreSQL:** Aseg칰rate de tener una instancia de PostgreSQL ejecut치ndose localmente y accesible. La API crear치 las tablas autom치ticamente al iniciar.
2.  **Variables de Entorno:** Crea tu archivo `.env` a partir del `.env.example` y rellena las siguientes variables:
    *   `DATABASE_URL`: La URL de conexi칩n a tu base de datos PostgreSQL local. Ejemplo:
        ```
        DATABASE_URL="postgresql+asyncpg://user:password@localhost:5432/your_database_name"
        ```
    *   `OPENAI_API_KEY`, `PINECONE_API_KEY`, `PINECONE_INDEX_NAME`: Tus credenciales.
    *   `API_BASE_URL`: Para la comunicaci칩n entre Streamlit y la API local. Debe ser:
        ```
        API_BASE_URL=http://localhost:8000/api/v1
        ```
        (Nota: Esta variable se sobreescribe en Docker Compose).

3.  **Instalar uv (Recomendado)**: La forma m치s r치pida de configurar el entorno es con `uv`. Se recomienda instalarlo en tu entorno global de Python.
    ```bash
    pip install uv
    ```
    Luego, desde la ra칤z del proyecto, ejecuta:
    ```bash
    uv sync
    ```
    Este comando leer치 el `pyproject.toml`, usar치 el `uv.lock` para instalar las versiones exactas de las dependencias y crear치 un entorno virtual en `.venv` si no existe.

4.  **Alternativa (pip + venv)**: Si prefieres no usar `uv`, puedes seguir el m칠todo tradicional:
    ```bash
    # Crear y activar un entorno virtual
    python -m venv .venv
    source .venv/bin/activate
    # Instalar dependencias
    pip install -r requirements.txt
    ```

5.  **Activar el Entorno Virtual**: Antes de continuar, aseg칰rate de que el entorno est칠 activado:
    ```bash
    source .venv/bin/activate
    ```

6.  **Generar Embeddings (Solo si no est치n pre-generados):** Si est치s configurando el proyecto desde cero y tu base de datos vectorial est치 vac칤a, necesitar치s generar los embeddings. Aseg칰rate de que tus variables de entorno de Pinecone y OpenAI est칠n configuradas, y luego ejecuta:
    ```bash
    python src/rag/init.py
    ```
    Este script descargar치 el contenido de Wikipedia, lo procesar치 y lo subir치 a tu 칤ndice de Pinecone.

#### 1. Iniciar la API (Backend)

Con el entorno virtual activado, inicia el servidor de la API en modo desarrollo:
```bash
fastapi dev src/api/main.py
```
La API estar치 escuchando en `http://localhost:8000`.

#### 2. Iniciar Streamlit (Frontend)

En una **nueva terminal**, activa el mismo entorno virtual:
```bash
source .venv/bin/activate
```
Luego, ejecuta la aplicaci칩n de Streamlit:
```bash
streamlit run streamlit_app/app.py
```
La interfaz de usuario estar치 disponible en `http://localhost:8501`.


## Arquitectura del Proyecto

### Flujo RAG (Retrieval-Augmented Generation)

El n칰cleo del chatbot es un sistema RAG orquestado por **LangChain**. Los archivos clave se encuentran en `src/rag/`:

*   `data_extractor.py`: Extrae el contenido de texto desde la p치gina de Wikipedia sobre Colombia.
*   `text_processor.py`: Limpia y divide el texto en fragmentos (`chunks`).
*   `embeddings.py`: Utiliza **OpenAI text-embedding-3-small** para convertir cada fragmento en un vector.
*   `vector_store.py`: Almacena y gestiona los vectores en una base de datos vectorial de **Pinecone**, permitiendo b칰squedas de similitud eficientes.

### API (FastAPI)

La API expone la l칩gica del chatbot y gestiona las conversaciones.

#### Endpoint Principal

*   `POST /api/v1/chat/`
    Este endpoint es el coraz칩n de la interacci칩n. Recibe una pregunta y, opcionalmente, un `conversation_id`. Si no se proporciona un ID, se crea una nueva conversaci칩n autom치ticamente.
    *   **Modelo Principal:** **OpenAI gpt-4o** genera la respuesta final bas치ndose en el contexto recuperado de Pinecone.
    *   **Modelo de Apoyo:** **OpenAI gpt-4o-mini** reformula la pregunta del usuario para incluir el contexto de mensajes anteriores, mejorando la coherencia.

#### Endpoints de Conversaci칩n

*   `POST /api/v1/conversations/`
*   `GET /api/v1/conversations/`
*   `GET /api/v1/conversations/{conversation_id}`

Estos endpoints hacen que el chatbot sea *stateful*, permitiendo crear, listar y recuperar conversaciones. La informaci칩n se almacena en una base de datos PostgreSQL.

### Interfaz de Usuario (Streamlit)

La carpeta `streamlit_app/` contiene la interfaz web interactiva que act칰a como cliente de la API, permitiendo a los usuarios chatear y gestionar sus conversaciones.

### Docker

El proyecto est치 completamente dockerizado para un despliegue sencillo y portable.
*   `Dockerfile`: Define la imagen para la API de FastAPI.
*   `Dockerfile.streamlit`: Define la imagen para la interfaz de Streamlit.
*   `docker-compose.yml`: Orquesta la ejecuci칩n de ambos servicios, sus redes y configuraciones.

## Tests

El proyecto incluye tests unitarios para asegurar la funcionalidad de los componentes clave. Puedes ejecutar todos los tests desde la ra칤z del proyecto usando `pytest`:

```bash
pytest
```

Los tests se encuentran en la carpeta `tests/` y est치n organizados de la siguiente manera:

*   `tests/api/test_endpoints.py`: Contiene tests para los endpoints de la API.
*   `tests/rag/test_data_extractor.py`: Contiene tests para el m칩dulo de extracci칩n de datos RAG.